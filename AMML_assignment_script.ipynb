{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pawij/amml-2526"
      ],
      "metadata": {
        "id": "kzpWR0gn79Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/amml-2526/src')"
      ],
      "metadata": {
        "id": "DhOoDz-K8eEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7hw8A1gwlUy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from model import VariationalAutoencoder\n",
        "\n",
        "use_gpu = True\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model0 = VariationalAutoencoder()\n",
        "model0.load_state_dict(torch.load('/content/amml-2526/data/amml_model0_weights.pth', weights_only=True))\n",
        "model0 = model0.to(device)\n",
        "model0.eval()\n",
        "\n",
        "model1 = VariationalAutoencoder()\n",
        "model1.load_state_dict(torch.load('/content/amml-2526/data/amml_model1_weights.pth', weights_only=True))\n",
        "model1 = model1.to(device)\n",
        "model1.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = MNIST(root='./data/MNIST', download=True, train=True, transform=img_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = MNIST(root='./data/MNIST', download=True, train=False, transform=img_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "qYuIbEgzzGpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "import torchvision.utils\n",
        "\n",
        "# This function takes as an input the images to reconstruct\n",
        "# and the name of the model with which the reconstructions\n",
        "# are performed\n",
        "def to_img(x):\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "\n",
        "def show_image(img):\n",
        "    img = to_img(img)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def visualise_output(images, model):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        images = images.to(device)\n",
        "        images, _, _ = model(images)\n",
        "        images = images.cpu()\n",
        "        images = to_img(images)\n",
        "        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()\n",
        "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "for images, labels in test_dataloader:\n",
        "  break\n",
        "\n",
        "# First visualise the original images\n",
        "print('Original images')\n",
        "show_image(torchvision.utils.make_grid(images[1:50],10,5))\n",
        "plt.show()\n",
        "\n",
        "# Reconstruct and visualise the images using the models"
      ],
      "metadata": {
        "id": "V5lCKRRtyivM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}